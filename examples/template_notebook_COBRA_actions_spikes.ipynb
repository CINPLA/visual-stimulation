{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# register and preprocess\n",
    "Run from here (as `!expipe register etc.`) or terminal (without exclamation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!expipe register openephys /Users/ehagen/COBRA/COBRA_exp_data/1010_2018-10-29_18-02-16_1/ --user Espen --location expipehell --overwrite\n",
    "!expipe register process --probe-path /Users/ehagen/COBRA/COBRA_exp_data/neuronexus-32-linear-list.prb --sorter ironclust 1010-291018-1\n",
    "!expipe register psychopy 1010-291018-1\n",
    "!expipe register mousexy  1010-291018-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import expipe\n",
    "import os\n",
    "from expipe_plugin_cinpla.imports import project\n",
    "#project = expipe_plugin_cinpla.imports.project\n",
    "import quantities as pq\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import h5py\n",
    "import neo\n",
    "import exdir\n",
    "import exdir.plugins.git_lfs # stupid\n",
    "#from exana.statistics import correlogram\n",
    "import pandas\n",
    "import elephant\n",
    "import scipy.signal as ss\n",
    "import elephant.current_source_density as csd\n",
    "from skimage.measure import block_reduce\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenlist = lambda lst: [item for sublist in lst for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams.update(**plt.rcParamsDefault)\n",
    "plt.rcParams.update({\n",
    "    #'figure.dpi' : 150,\n",
    "    'figure.figsize' : [6.4*1.5, 4.8*1.5],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expipe.config.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_id = '1006-121018-03'\n",
    "#action_id = '1011-301018-1' # gratin_quick\n",
    "#action_id = '1012-090119-01' # flashes_quick\n",
    "#action_id = '1012-090119-02' # gratings_quick\n",
    "#action_id = '1012-090119-04' # flashes_quick\n",
    "#action_id = '1012-090119-05' # gratings_quick\n",
    "action_id = '1012-090119-06' # gratings\n",
    "#action_id = '1012-090119-07' # images\n",
    "#action_id = '1012-090119-08' # sparsenoise\n",
    "#action_id = '1012-090119-09'  # static gratings\n",
    "#action_id = '1012-090119-10' # videos\n",
    "#action_id = '1012-090119-11' # flashes_quick\n",
    "#action_id = '1012-090119-11' # gratings_quick\n",
    "#project_id = PAR.PROJECT_ID\n",
    "#project = PAR.PROJECT\n",
    "action = project.actions[action_id]\n",
    "from expipe_plugin_cinpla.scripts.utils import _get_data_path\n",
    "exdir_path = _get_data_path(action)\n",
    "channel_group = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = ANALYSIS_PARAMS = {\n",
    "    'speed_filter': 5 * pq.m / pq.s,\n",
    "    'pos_fs': 100 * pq.Hz,\n",
    "    'f_cut': 6 * pq.Hz,\n",
    "    'spat_binsize': 0.02 * pq.m,\n",
    "    'spat_smoothing': 0.025,\n",
    "    'grid_stepsize': 0.1 * pq.m,\n",
    "    'box_xlen': 1 * pq.m,\n",
    "    'box_ylen': 1 * pq.m,\n",
    "    'ang_binsize': 4,\n",
    "    'ang_n_avg_bin': 4,\n",
    "    'imgformat': '.png',\n",
    "    'corr_bin_width': 0.001 * pq.s,\n",
    "    'corr_limit': 0.1 * pq.s,\n",
    "    'isi_binsize': 1 * pq.ms,\n",
    "    'isi_time_limit': 100 * pq.ms,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimulus epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = exdir.File(exdir_path, 'r', plugins=[exdir.plugins.quantities, exdir.plugins.git_lfs])\n",
    "stim = f['processing']['epochs']['visual_stimulus']\n",
    "grp = stim[list(stim.keys())[0]]\n",
    "if list(stim.keys()) == ['image']:\n",
    "    stimulation_type = 'image'\n",
    "    # nicer label formatting\n",
    "    labels = np.array([pathlib.PureWindowsPath(lbl).parts[-1].rstrip('.png') for lbl in grp['image'].value])\n",
    "    epo=neo.Epoch(times=grp['times'].value, \n",
    "                  durations=grp['duration'].value*pq.s, \n",
    "                  labels=labels)\n",
    "elif list(stim.keys()) == ['grating']:\n",
    "    stimulation_type = 'grating'\n",
    "    annotations = dict(frequency=grp['frequency'].value*pq.Hz,\n",
    "                       orientation=grp['orientation'].value*pq.deg,\n",
    "                       phase=grp['phase'].value,\n",
    "                       spatial_frequency=grp['spatial_frequency'].value\n",
    "                      )\n",
    "    df = pandas.DataFrame(annotations)\n",
    "    # nicer label formatting\n",
    "    labels = np.array([df.loc[i].to_csv().replace(',', ':').replace('\\n', ',') for i in range(df.shape[0])])\n",
    "    labels = [label.replace('spatial_frequency:', '\\omega=') for label in labels]\n",
    "    labels = [label.replace('frequency:', 'f=') for label in labels]\n",
    "    labels = [label.replace('orientation:', r'\\alpha=') for label in labels]\n",
    "    labels = [label.replace('phase:', r'\\theta=') for label in labels]\n",
    "    labels = np.array([r'${}$'.format(label) for label in labels])\n",
    "    epo=neo.Epoch(times=grp['times'].value, \n",
    "                  durations=grp['duration'].value*pq.s, \n",
    "                  labels=labels,\n",
    "                  **annotations)\n",
    "elif list(stim.keys()) == ['sparsenoise']:\n",
    "    stimulation_type = 'sparsenoise'\n",
    "    epo=neo.Epoch(times=grp['times'].value, \n",
    "                  durations=grp['duration'].value*pq.s, \n",
    "                  labels=grp['image'].value)\n",
    "elif list(stim.keys()) == ['movie']:\n",
    "    stimulation_type = 'movie'\n",
    "    # nicer label formatting\n",
    "    labels = np.array([pathlib.PureWindowsPath(lbl).parts[-1].rstrip('.mp4') for lbl in grp['movie'].value])\n",
    "    epo=neo.Epoch(times=grp['times'].value, \n",
    "                  durations=np.array([30.]*grp['times'].size)*pq.s, #### HACK!!!! ###### \n",
    "                  labels=labels)\n",
    "elif list(stim.keys()) == ['flash']:\n",
    "    stimulation_type = 'flash'\n",
    "    epo=neo.Epoch(times=grp['times'].value, \n",
    "                  durations=(grp['duration'].value + 0.25)*pq.s, \n",
    "                  labels=grp['color'].value)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulation_type , epo.times, epo.durations, epo.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikes\n",
    "Load all spiketrains as list of `neo.SpikeTrain` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_waveforms = True\n",
    "if load_waveforms:\n",
    "    f = exdir.File(exdir_path, 'r', plugins=[exdir.plugins.quantities, exdir.plugins.git_lfs])\n",
    "    io = neo.ExdirIO(str(exdir_path))\n",
    "    blk = io.read_block()\n",
    "    blk_cluster_ids = np.array([str(unit.annotations['cluster_id']) for unit in blk.channel_indexes[0].units])\n",
    "    t_stop = f.attrs['session_duration']\n",
    "    UnitTimes = f['processing']['electrophysiology']['channel_group_0']['UnitTimes']\n",
    "    units = np.sort(list(UnitTimes.keys()))\n",
    "    spiketrains = []\n",
    "    for unit in units:\n",
    "        if not UnitTimes[unit].attrs['cluster_group'] == 'noise':\n",
    "            spiketrains += [neo.SpikeTrain(UnitTimes[unit]['times'].value, t_stop=t_stop, \n",
    "                                          name=unit, description=UnitTimes[unit].attrs['cluster_group'],\n",
    "                                          waveforms=blk.segments[0].spiketrains[np.where(blk_cluster_ids==unit)[0][0]].waveforms)]\n",
    "    f.close()\n",
    "else:\n",
    "    f = exdir.File(exdir_path, 'r', plugins=[exdir.plugins.quantities, exdir.plugins.git_lfs])\n",
    "    t_stop = f.attrs['session_duration']\n",
    "    UnitTimes = f['processing']['electrophysiology']['channel_group_0']['UnitTimes']\n",
    "    units = np.sort(list(UnitTimes.keys()))\n",
    "    spiketrains = [neo.SpikeTrain(UnitTimes[unit]['times'].value, t_stop=t_stop, \n",
    "                                  name=unit, description=UnitTimes[unit].attrs['cluster_group'],\n",
    "                                  waveforms=None) \n",
    "                   for unit in units if not UnitTimes[unit].attrs['cluster_group'] == 'noise']\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def some plotting functions\n",
    "def remove_axis_junk(ax, lines=['right', 'top']):\n",
    "    for loc, spine in ax.spines.items():\n",
    "        if loc in lines:\n",
    "            spine.set_color('none')            \n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "def draw_lineplot(\n",
    "        ax, data, dt=0.1,\n",
    "        T=(0, 200),\n",
    "        scaling_factor=1.,\n",
    "        vlimround=None,\n",
    "        label='local',\n",
    "        scalebar=True,\n",
    "        unit='mV',\n",
    "        ylabels=True,\n",
    "        color='k',\n",
    "        ztransform=True,\n",
    "        filter=False,\n",
    "        filterargs=dict(N=2, Wn=0.02, btype='lowpass')\n",
    "        ):\n",
    "    ''' draw some nice lines'''\n",
    "    \n",
    "    tvec = np.arange(data.shape[1])*dt\n",
    "    if T[0] < 0:\n",
    "        tvec += T[0]\n",
    "    try:\n",
    "        tinds = (tvec >= T[0]) & (tvec <= T[1])\n",
    "    except TypeError:\n",
    "        print(data.shape, T)\n",
    "        raise Exception\n",
    "    \n",
    "    # apply temporal filter\n",
    "    if filter:\n",
    "        b, a = ss.butter(**filterargs)\n",
    "        data = ss.filtfilt(b, a, data, axis=-1)\n",
    "    \n",
    "    #subtract mean in each channel\n",
    "    if ztransform:\n",
    "        dataT = data.T - data.mean(axis=1)\n",
    "        data = dataT.T\n",
    "\n",
    "    zvec = np.arange(data.shape[0])\n",
    "    vlim = abs(data[:, tinds]).max()\n",
    "    if vlimround is None:\n",
    "        vlimround = 2.**np.round(np.log2(vlim)) / scaling_factor\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    yticklabels=[]\n",
    "    yticks = []\n",
    "    \n",
    "    for i, z in enumerate(zvec):\n",
    "        if i == 0:\n",
    "            ax.plot(tvec[tinds], data[i][tinds] / vlimround + z, lw=.5,\n",
    "                    rasterized=False, label=label, clip_on=False,\n",
    "                    color=color)\n",
    "        else: \n",
    "            ax.plot(tvec[tinds], data[i][tinds] / vlimround + z, lw=.5,\n",
    "                    rasterized=False, clip_on=False,\n",
    "                    color=color)\n",
    "        yticklabels.append('ch. %i' % (i+1))\n",
    "        yticks.append(z)\n",
    "        \n",
    "    if scalebar:\n",
    "        ax.plot([tvec[tinds][-1], tvec[tinds][-1]],\n",
    "                [zvec[-1], zvec[-2]], lw=2, color='k', clip_on=False)\n",
    "        ax.text(tvec[tinds][-1]+np.diff(T)*0.0, np.mean([zvec[-1], zvec[-2]]),\n",
    "                '$2^{' + '{}'.format(int(np.log2(vlimround))) + '}$ ' + '{0}'.format(unit),\n",
    "                color='r', rotation='vertical',\n",
    "                va='center', zorder=100)\n",
    "\n",
    "    ax.axis(ax.axis('tight'))\n",
    "    if ylabels:\n",
    "        ax.yaxis.set_ticks(yticks)\n",
    "        ax.yaxis.set_ticklabels(yticklabels)\n",
    "        #ax.set_ylabel('channel', labelpad=0.1)\n",
    "    else:\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "    remove_axis_junk(ax, lines=['right', 'top'])\n",
    "    ax.set_xlabel(r'time (ms)', labelpad=0.1)\n",
    "    \n",
    "    return vlimround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_waveforms:\n",
    "    vlimround=2**6\n",
    "    fig, axes = plt.subplots(1, len(spiketrains), sharey=True)\n",
    "    for i, (ax, spiketrain) in enumerate(zip(axes, spiketrains)):\n",
    "        mean = spiketrain.waveforms.mean(axis=0)\n",
    "        std = spiketrain.waveforms.std(axis=0)\n",
    "        draw_lineplot(ax, mean, dt=1./30, T=(-0.5, 1.5), vlimround=vlimround, unit=mean.dimensionality)\n",
    "        draw_lineplot(ax, mean+std*2, dt=1./30, T=(-.5, 1.5), vlimround=vlimround, scalebar=False, color='0.5')\n",
    "        draw_lineplot(ax, mean-std*2, dt=1./30, T=(-.5, 1.5), vlimround=vlimround, scalebar=False, color='0.5')\n",
    "        ax.set_title('#{}'.format(spiketrain.name))\n",
    "        if i != 0:\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MouseXY tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = exdir.File(exdir_path, 'r', plugins=[exdir.plugins.quantities])\n",
    "tracks = dict()\n",
    "for key, value in f['processing']['tracking']['trackball']['position'].items():\n",
    "    tracks[key] = value\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiketrain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_plot(ax, spiketrains, T=[0., 10.], epo=None):\n",
    "    '''\n",
    "    Arguments\n",
    "    ---------\n",
    "    ax : matplotlib.axes._subplots.AxesSubplot\n",
    "    spiketrains : list of neo.SpikeTrain objects\n",
    "    T : length 2 list/tuple of floats\n",
    "        time interval in seconds\n",
    "    epo : None or neo.Epoch object\n",
    "        show onset/offset times of stimuli\n",
    "    '''\n",
    "    yticklabels = []\n",
    "    for i, spiketrain in enumerate(spiketrains):\n",
    "        yticklabels.append('{} ({})'.format(spiketrain.name, spiketrain.description))\n",
    "        ax.plot(spiketrain, np.zeros(spiketrain.size)+i, 'C0|')\n",
    "    if epo is not None:\n",
    "        axis = ax.axis('tight')\n",
    "        ax.vlines(epo.times, axis[2], axis[3], 'g')\n",
    "        ax.vlines((epo.times+epo.durations), axis[2], axis[3], 'r')        \n",
    "    ax.set_yticks(range(len(spiketrains)))\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_ylabel('unit id')\n",
    "    ax.set_xlim(T)\n",
    "    ax.set_xlabel('t (s)')\n",
    "    ax.set_title('spike raster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "raster_plot(ax, spiketrains, T=[30., 40.], epo=epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interspike interval (ISI) distributions (log-linear bins)\n",
    "nrows = int(np.ceil(np.sqrt(len(spiketrains))))\n",
    "ncols = int(np.ceil(len(spiketrains) / nrows))\n",
    "fig = plt.figure()\n",
    "gs = GridSpec(nrows, ncols)\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "bins = 10**np.linspace(-3, 1, 51) # 50 log-lin bins between 1 ms and 10 s\n",
    "labels = [spiketrain.name for spiketrain in spiketrains]\n",
    "for i, (sptr, name) in enumerate(zip(spiketrains, labels)):\n",
    "    ax = fig.add_subplot(gs[i // ncols, i % ncols])\n",
    "    ax.hist(np.diff(sptr), bins=bins, color='g' if 'good' in name else None)\n",
    "    ax.semilogx()\n",
    "    ax.set_xlim(bins.min(), bins.max())\n",
    "    ax.set_title('unit {} ({})'.format(sptr.name, sptr.description))\n",
    "    if i % ncols == 0:\n",
    "        ax.set_ylabel('#')\n",
    "    if i  < len(spiketrains) - ncols:\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel('ISI (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interspike interval (ISI) distributions (linear bins)\n",
    "nrows = int(np.ceil(np.sqrt(len(spiketrains))))\n",
    "ncols = int(np.ceil(len(spiketrains) / nrows))\n",
    "fig = plt.figure()\n",
    "gs = GridSpec(nrows, ncols)\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "bins = np.linspace(1, 50, 51) # 50 log-lin bins between 1 ms and 50 ms\n",
    "for i, (sptr, name) in enumerate(zip(spiketrains, labels)):\n",
    "    ax = fig.add_subplot(gs[i // ncols, i % ncols])\n",
    "    ax.hist(np.diff(sptr)*1E3, bins=bins, color='g' if 'good' in name else None)\n",
    "    ax.set_title('unit {} ({})'.format(sptr.name, sptr.description))\n",
    "    if i % ncols == 0:\n",
    "        ax.set_ylabel('#')\n",
    "    if i  < len(spiketrains) - ncols:\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel('ISI (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlogram(t1, t2=None, binsize=.001, limit=.02, auto=False,\n",
    "                density=False):\n",
    "    \"\"\"Return crosscorrelogram of two spike trains.\n",
    "    Essentially, this algorithm subtracts each spike time in `t1`\n",
    "    from all of `t2` and bins the results with np.histogram, though\n",
    "    several tweaks were made for efficiency.\n",
    "    Originally authored by Chris Rodger, copied from OpenElectrophy, licenced\n",
    "    with CeCill-B. Examples and testing written by exana team.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    t1 : np.array, or neo.SpikeTrain\n",
    "        First spiketrain, raw spike times in seconds.\n",
    "    t2 : np.array, or neo.SpikeTrain\n",
    "        Second spiketrain, raw spike times in seconds.\n",
    "    binsize : float, or quantities.Quantity\n",
    "        Width of each bar in histogram in seconds.\n",
    "    limit : float, or quantities.Quantity\n",
    "        Positive and negative extent of histogram, in seconds.\n",
    "    auto : bool\n",
    "        If True, then returns autocorrelogram of `t1` and in\n",
    "        this case `t2` can be None. Default is False.\n",
    "    density : bool\n",
    "        If True, then returns the probability density function.\n",
    "    See also\n",
    "    --------\n",
    "    :func:`numpy.histogram` : The histogram function in use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (count, bins) : tuple\n",
    "        A tuple containing the bin right edges and the\n",
    "        count/density of spikes in each bin.\n",
    "    Note\n",
    "    ----\n",
    "    `bins` are relative to `t1`. That is, if `t1` leads `t2`, then\n",
    "    `count` will peak in a positive time bin.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> t1 = np.arange(0, .5, .1)\n",
    "    >>> t2 = np.arange(0.1, .6, .1)\n",
    "    >>> limit = 1\n",
    "    >>> binsize = .1\n",
    "    >>> counts, bins = correlogram(t1=t1, t2=t2, binsize=binsize,\n",
    "    ...                            limit=limit, auto=False)\n",
    "    >>> counts\n",
    "    array([0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 4, 3, 2, 1, 0, 0, 0, 0])\n",
    "\n",
    "    The interpretation of this result is that there are 5 occurences where\n",
    "    in the bin 0 to 0.1, i.e.\n",
    "\n",
    "    # TODO fix\n",
    "    # >>> idx = np.argmax(counts)\n",
    "    # >>> '%.1f, %.1f' % (abs(bins[idx - 1]), bins[idx])\n",
    "    # '0.0, 0.1'\n",
    "\n",
    "    The correlogram algorithm is identical to, but computationally faster than\n",
    "    the histogram of differences of each timepoint, i.e.\n",
    "\n",
    "    # TODO Fix the doctest\n",
    "    # >>> diff = [t2 - t for t in t1]\n",
    "    # >>> counts2, bins = np.histogram(diff, bins=bins)\n",
    "    # >>> np.array_equal(counts2, counts)\n",
    "    # True\n",
    "    \"\"\"\n",
    "    if auto: t2 = t1\n",
    "    # For auto-CCGs, make sure we use the same exact values\n",
    "    # Otherwise numerical issues may arise when we compensate for zeros later\n",
    "\n",
    "    if not int(limit * 1e10) % int(binsize * 1e10) == 0:\n",
    "        raise ValueError(\n",
    "            'Time limit {} must be a '.format(limit) +\n",
    "            'multiple of binsize {}'.format(binsize) +\n",
    "            ' remainder = {}'.format(limit % binsize))\n",
    "    # For efficiency, `t1` should be no longer than `t2`\n",
    "    swap_args = False\n",
    "    if len(t1) > len(t2):\n",
    "        swap_args = True\n",
    "        t1, t2 = t2, t1\n",
    "\n",
    "    # Sort both arguments (this takes negligible time)\n",
    "    t1 = np.sort(t1)\n",
    "    t2 = np.sort(t2)\n",
    "\n",
    "    # Determine the bin edges for the histogram\n",
    "    # Later we will rely on the symmetry of `bins` for undoing `swap_args`\n",
    "    limit = float(limit)\n",
    "\n",
    "    # The numpy.arange method overshoots slightly the edges i.e. binsize + epsilon\n",
    "    # which leads to inclusion of spikes falling on edges.\n",
    "    bins = np.arange(-limit, limit + binsize, binsize)\n",
    "\n",
    "    # Determine the indexes into `t2` that are relevant for each spike in `t1`\n",
    "    ii2 = np.searchsorted(t2, t1 - limit)\n",
    "    jj2 = np.searchsorted(t2, t1 + limit)\n",
    "\n",
    "    # Concatenate the recentered spike times into a big array\n",
    "    # We have excluded spikes outside of the histogram range to limit\n",
    "    # memory use here.\n",
    "    big = np.concatenate([t2[i:j] - t for t, i, j in zip(t1, ii2, jj2)])\n",
    "\n",
    "    # Actually do the histogram. Note that calls to np.histogram are\n",
    "    # expensive because it does not assume sorted data.\n",
    "    count, bins = np.histogram(big, bins=bins, density=density)\n",
    "\n",
    "    if auto:\n",
    "        # Compensate for the peak at time zero that results in autocorrelations\n",
    "        # by subtracting the total number of spikes from that bin. Note\n",
    "        # possible numerical issue here because 0.0 may fall at a bin edge.\n",
    "        c_temp, bins_temp = np.histogram([0.], bins=bins)\n",
    "        bin_containing_zero = np.nonzero(c_temp)[0][0]\n",
    "        count[bin_containing_zero] = 0#-= len(t1)\n",
    "\n",
    "    # Finally compensate for the swapping of t1 and t2\n",
    "    if swap_args:\n",
    "        # Here we rely on being able to simply reverse `counts`. This is only\n",
    "        # possible because of the way `bins` was defined (bins = -bins[::-1])\n",
    "        count = count[::-1]\n",
    "\n",
    "    return count, bins[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# spike-train cross-correlation histograms\n",
    "bin_width = par['corr_bin_width'].rescale('s').magnitude\n",
    "limit = par['corr_limit'].rescale('s').magnitude\n",
    "fig = plt.figure()\n",
    "gs = GridSpec(len(spiketrains), len(spiketrains))\n",
    "for i, t1 in enumerate(spiketrains):\n",
    "    for j, t2 in enumerate(spiketrains):\n",
    "        if j < i:\n",
    "            continue\n",
    "        else:\n",
    "            count, bins = correlogram(t1=t1.as_array(), t2=t2.as_array(),\n",
    "                                      binsize=bin_width, limit=limit,\n",
    "                                      auto=True if i == j else False)        \n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.bar(bins, count, width=bin_width)\n",
    "            ax.set_ylim(0, count.max())\n",
    "            ax.set_xlim([-limit, limit])\n",
    "            if i == 0:\n",
    "                ax.set_title('#{}'.format(t2.name), fontsize='medium')\n",
    "            if i == j:\n",
    "                ax.set_ylabel('#{}'.format(t1.name))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-unit responses to stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for spiketrain in spiketrains:\n",
    "    title = 'unit {} ({})'.format(spiketrain.name, spiketrain.description)\n",
    "\n",
    "    labels = np.unique(epo.labels)\n",
    "    ntrials = int(np.ceil(epo.labels.size / labels.size))\n",
    "    container = np.zeros((ntrials, labels.size))\n",
    "\n",
    "    # determine significance level assuming a stationary rate poisson process\n",
    "    #mean_rate = elephant.statistics.mean_firing_rate(spiketrain)\n",
    "        \n",
    "    for j, label in enumerate(labels):\n",
    "        inds = epo.labels == label\n",
    "        for i, (time, duration) in enumerate(zip(epo.times[inds], epo.durations[inds])):\n",
    "            container[i, j] += spiketrain.time_slice(time, time+duration).size\n",
    "    fig, axes = plt.subplots(3, 1, sharex=True)\n",
    "    fig.subplots_adjust(bottom=0.3)\n",
    "    ax = axes[0]\n",
    "    im = ax.pcolormesh(np.arange(labels.size+1)-0.5, np.arange(ntrials+1)-0.5, container, \n",
    "                       vmin=0, vmax=50, cmap='gray_r')\n",
    "    ax.axis(ax.axis('tight'))\n",
    "    ax.set_xticks(np.arange(labels.size))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_ylabel('trial #')\n",
    "    ax.set_title(title)\n",
    "    # total spike count\n",
    "    ax = axes[1]\n",
    "    ax.bar(np.arange(labels.size), container.sum(axis=0))\n",
    "    ax.set_ylabel('tot. spike count')\n",
    "    ax.axis(ax.axis('tight'))\n",
    "    # rate per stim\n",
    "    ax = axes[2]\n",
    "    ax.bar(np.arange(labels.size), (container.sum(axis=0) / epo.durations.sum()*labels.size).simplified)\n",
    "    mean_rate = (container.sum() / epo.durations.sum()).simplified    \n",
    "    ax.hlines(mean_rate, -1, len(labels), linestyle=':')\n",
    "    ax.set_ylabel('rate (1/s)')\n",
    "    ax.set_xticks(np.arange(labels.size))\n",
    "    ax.set_xticklabels(labels, rotation=90, fontsize='x-small')\n",
    "    ax.axis(ax.axis('tight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tuning curves\n",
    "if stimulation_type == 'grating':\n",
    "    for spiketrain in spiketrains:\n",
    "        title = 'unit {} ({})'.format(spiketrain.name, spiketrain.description)\n",
    "        labels = np.unique(epo.annotations['orientation'])\n",
    "        ntrials = int(np.ceil(epo.labels.size / labels.size))\n",
    "        container = np.zeros((ntrials, labels.size))\n",
    "\n",
    "        for j, label in enumerate(labels):\n",
    "            inds = epo.annotations['orientation'] == label\n",
    "            for i, (time, duration) in enumerate(zip(epo.times[inds], epo.durations[inds])):\n",
    "                container[i, j] += spiketrain.time_slice(time, time+duration).size\n",
    "        # determine significance level assuming a stationary rate poisson process\n",
    "        # mean_rate = elephant.statistics.mean_firing_rate(spiketrain)\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, sharex=True)\n",
    "        fig.subplots_adjust(bottom=0.2)\n",
    "        ax = axes[0]\n",
    "        im = ax.pcolormesh(np.arange(labels.size+1)-0.5, np.arange(ntrials+1)-0.5, \n",
    "                           container, vmin=0, vmax=20, cmap='gray_r')\n",
    "        ax.axis(ax.axis('tight'))\n",
    "        ax.set_xticks(np.arange(labels.size))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_ylabel('trial #')\n",
    "        ax.set_title(title)\n",
    "        ax = axes[1]\n",
    "        ax.bar(np.arange(labels.size), container.sum(axis=0))\n",
    "        #ax.hlines(mean_rate.magnitude, -1, len(labels), linestyle=':')\n",
    "        ax.set_xticks(np.arange(labels.size))\n",
    "        ax.set_xticklabels([str(label).split('\\\\')[-1] for label in labels], rotation=90, fontsize=12)\n",
    "        ax.set_ylabel('tot. spike count')\n",
    "        # rate per stim\n",
    "        ax = axes[2]\n",
    "        ax.bar(np.arange(labels.size), (container.sum(axis=0) / epo.durations.sum()*len(labels)).simplified)\n",
    "        mean_rate = (container.sum() / epo.durations.sum()).simplified  \n",
    "        ax.hlines(mean_rate.magnitude, -1, len(labels), linestyle=':')\n",
    "        ax.set_ylabel('rate (1/s)')\n",
    "        ax.set_xticks(np.arange(labels.size))\n",
    "        ax.set_xticklabels(labels, rotation=90, fontsize='x-small')\n",
    "        ax.axis(ax.axis('tight'))\n",
    "        \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate image sequence with binned spike trains\n",
    "if stimulation_type in ['sparsenoise']:\n",
    "    # load h5 file with image data\n",
    "    im_path = os.path.join(expipe_temp_storage, project_id, \n",
    "                          'COBRA_PsychoPy_files', 'files', 'datasets', \n",
    "                          'sparse_noise_images', 'imageData.h5')\n",
    "    f = h5py.File(im_path, 'r')\n",
    "    im_data = f['data'][:, :epo.size]\n",
    "    im_dimensions = f['dimensions'].value\n",
    "    f.close()\n",
    "    im_shape = im_data.shape\n",
    "    im_data = im_data.reshape((-1), im_data.shape[-1])\n",
    "    \n",
    "    for spiketrain in spiketrains:\n",
    "        title = 'unit {}'.format(spiketrain.name)\n",
    "        # count spike events per image\n",
    "        sptr_binned = np.zeros(im_data.shape[-1], dtype=int)\n",
    "        for i, (time, duration) in enumerate(zip(epo.times, epo.durations)):\n",
    "            sptr_binned[i] += spiketrain.time_slice(time, time+duration).size\n",
    "        corr = np.corrcoef(np.row_stack([sptr_binned, im_data]))[1:, 0].reshape(im_shape[:2])\n",
    "        plt.figure()\n",
    "        plt.imshow(corr, cmap='gray', vmin=-corr.std()*2, vmax=corr.std()*2, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlate image sequence with binned spike trains\n",
    "if stimulation_type in ['image']:\n",
    "    # load h5 file with image data\n",
    "    im_path = os.path.join(os.environ['HOME'], 'COBRA', \n",
    "                          'COBRA_PsychoPy_files', 'files', 'datasets', \n",
    "                          'converted_images', 'imageData.h5')\n",
    "\n",
    "    f = h5py.File(im_path, 'r')\n",
    "    im_data = f['data'].value\n",
    "    im_dimensions = f['dimensions'].value\n",
    "    f.close()\n",
    "    im_shape = im_data.shape\n",
    "    \n",
    "    # test that downsampling works\n",
    "    plt.matshow(im_data[:, :, 0], cmap='gray')\n",
    "    plt.title('original')\n",
    "    \n",
    "    \n",
    "    # downsample image (too high-res for correlation calc)\n",
    "    im_data = block_reduce(im_data, block_size=(10, 10, 1), func=np.mean)\n",
    "    plt.matshow(im_data[:, :, 0], cmap='gray')\n",
    "    plt.title('downsampled')\n",
    "    im_shape = im_data.shape\n",
    "    im_data = im_data.reshape((-1), im_data.shape[-1])\n",
    "    \n",
    "    # epoch bins\n",
    "    bins = np.r_[epo.times, [epo.times[-1]+epo.durations[-1]]]\n",
    "    for spiketrain in spiketrains:\n",
    "        title = 'unit {} ({})'.format(spiketrain.name, spiketrain.description)\n",
    "        # count spike events per image showing\n",
    "        sptr_binned_tmp, _ = np.histogram(spiketrain.times, bins=bins)\n",
    "        sptr_binned = np.zeros(im_shape[-1], dtype=int)\n",
    "        # collapse bins with the same image\n",
    "        for i, label in enumerate(np.unique(epo.labels)):\n",
    "            inds = epo.labels == label\n",
    "            sptr_binned[i] = sptr_binned_tmp[inds].sum()\n",
    "        corr = np.corrcoef(np.row_stack([sptr_binned, im_data]))[1:, 0].reshape(im_shape[:2])\n",
    "        plt.figure()\n",
    "        plt.imshow(corr, cmap='gray', vmin=-corr.std()*2, vmax=corr.std()*2, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate movie data with binned spike trains\n",
    "if stimulation_type in ['movie']:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSTH\n",
    "(peristimulus time histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_psth(st, epoch, fig=None, axes=None, lags=(-0.1*pq.s, 1*pq.s), bin_size=0.01*pq.s, \n",
    "              marker='|', color='C0', n_trials=10,  histtype='bar'):\n",
    "    '''\n",
    "    Parameters:\n",
    "    st : neo.SpikeTrain\n",
    "    epoch : neo.Epoch\n",
    "    lags : tuple of Quantity scalars\n",
    "    bin_size : Quantity scalar\n",
    "    color : mpl color \n",
    "    n_trials : int\n",
    "        number of trials to include in PSTH\n",
    "    '''\n",
    "    labels = np.unique(epoch.labels, axis=-1)\n",
    "    bins = np.linspace(lags[0], lags[1], int((lags[1]-lags[0])//bin_size)+1)\n",
    "    \n",
    "    if fig is None:\n",
    "        fig, axes = plt.subplots(2, len(labels), sharex=True, sharey='row')\n",
    "        fig.suptitle('unit {} ({})'.format(st.name, st.description))\n",
    "    for i, label in enumerate(labels):\n",
    "        axes[0, i].set_xlim(lags)\n",
    "        axes[1, i].set_xlim(lags)\n",
    "\n",
    "        sts = []\n",
    "        for h, epo in enumerate(epoch[epoch.labels == label]):\n",
    "            if h < n_trials:\n",
    "                st_ = st.time_slice(t_start=(epo+lags[0]).simplified, \n",
    "                                    t_stop=(epo+lags[1]).simplified)\n",
    "                sts.append((st_.times.simplified - epo.simplified).tolist())\n",
    "                axes[0, i].plot(sts[h], np.zeros(len(sts[h])) + h, marker, color=color)\n",
    "\n",
    "        axes[0, i].set_title('{}'.format(label), fontsize='x-small')\n",
    "        axes[1, i].set_xlabel('lag (s)')\n",
    "        axes[1, i].hist(flattenlist(sts), bins=bins, color=color, histtype=histtype)\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel('trial #')\n",
    "            axes[1, i].set_ylabel('#')\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if stimulation_type in ['image', 'flash']:\n",
    "    for st in spiketrains:\n",
    "        plot_psth(st, epo, lags=(-0.1*pq.s, epo.durations[0]), n_trials=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
